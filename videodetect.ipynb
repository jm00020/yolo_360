{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "import pickle as pkl\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available() and use_cuda\n",
    "print(cuda)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"D:/Downloads/PyTorch-YOLOv3-master (2)/PyTorch-YOLOv3-master/data/samples\"\n",
    "video_folder = \"D:/lidar/test3.264\"\n",
    "config_path = \"config/yolov3.cfg\"\n",
    "weights_path = \"weights/yolov3.weights\"\n",
    "class_path = \"D:/Downloads/PyTorch-YOLOv3-master (2)/PyTorch-YOLOv3-master/data/coco.names\"\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 8\n",
    "img_size = 416\n",
    "use_cuda = True\n",
    "\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMap(Ws,Hs,Wd,Hd,R1,R2,Cx,Cy):\n",
    "    map_x = np.zeros((Hd,int(Wd)),np.float32)\n",
    "    map_y = np.zeros((Hd,int(Wd)),np.float32)\n",
    "    for y in range(0,int(Hd-1)):\n",
    "        for x in range(0,int(Wd-1)):\n",
    "            r = (float(y)/float(Hd))*(R2-R1)+R1\n",
    "            theta = (float(x)/float(Wd))*2.0*np.pi\n",
    "            xS = Cx+r*np.sin(theta)\n",
    "            yS = Cy+r*np.cos(theta)\n",
    "            map_x.itemset((y,x),int(xS))\n",
    "            map_y.itemset((y,x),int(yS))\n",
    "        \n",
    "    return map_x, map_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwarp(img,xmap,ymap):\n",
    "    output = cv2.remap(np.asarray(img),xmap,ymap,cv2.INTER_LINEAR)\n",
    "    result = Image.fromarray(output)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def omni_rect(img):\n",
    "    Cx = 208\n",
    "    Cy = 208\n",
    "    R1x = 250\n",
    "    R1y = 208\n",
    "    R1 = R1x-Cx\n",
    "    R2x = 416\n",
    "    R2y = 208\n",
    "    R2 = R2x-Cx\n",
    "    Wd = 2.0*((R2+R1)/2)*np.pi\n",
    "    Hd = (R2-R1)\n",
    "    Ws = 416\n",
    "    Hs = 416\n",
    "\n",
    "    xmap,ymap = buildMap(Ws,Hs,Wd,Hd,R1,R2,Cx,Cy)\n",
    "\n",
    "    result = unwarp(img,xmap,ymap)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "\n",
      "capture\n",
      "\n",
      "ImageDataLoader\n",
      "\n",
      "Performing object detection:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0:00:22.101774\n",
      "Inference Time: 0:00:00.038103\n",
      "Inference Time: 0:00:00.039039\n",
      "Inference Time: 0:00:00.042905\n",
      "Inference Time: 0:00:00.031232\n",
      "Inference Time: 0:00:00.037703\n",
      "Inference Time: 0:00:00.031232\n",
      "Inference Time: 0:00:00.030256\n",
      "Inference Time: 0:00:00.032569\n",
      "Inference Time: 0:00:00.033183\n",
      "Inference Time: 0:00:00.035182\n",
      "Inference Time: 0:00:00.033560\n",
      "Inference Time: 0:00:00.034196\n",
      "\n",
      "Saving images:\n",
      "\t+ Label: person, Conf: 0.97425\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99861\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99960\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99912\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99858\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99906\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99946\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99962\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99730\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99835\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99956\n",
      "event\n",
      "\t+ Label: person, Conf: 0.99728\n",
      "event\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available() and use_cuda\n",
    "\n",
    "model = Darknet(config_path, img_size)\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "\n",
    "classes = load_classes(class_path)\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    \n",
    "img_detections = []\n",
    "imgs = []\n",
    "videofile = video_folder\n",
    "    \n",
    "cap = cv2.VideoCapture(videofile, cv2.CAP_FFMPEG)\n",
    "\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    \n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "    \n",
    "count = 0;\n",
    "\n",
    "print('\\ncapture')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if count == cap.get(cv2.CAP_PROP_POS_FRAMES):\n",
    "        break\n",
    "    count = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    if (count % 22) == 0:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame)\n",
    "        img = img.resize((img_size, img_size))\n",
    "#         img = omni_rect(img)\n",
    "        imgs.append(img)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "print('\\nImageDataLoader')        \n",
    "dataloader = DataLoader(VideoLoader(imgs, img_size), batch_size, shuffle=False,num_workers=n_cpu)\n",
    "print('\\nPerforming object detection:')\n",
    "prev_time = time.time()    \n",
    "\n",
    "for batch_i, input_imgs in enumerate(dataloader):\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "            \n",
    "    current_time = time.time()\n",
    "    inferece_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    print('Inference Time: %s' % (inferece_time))\n",
    "\n",
    "    img_detections.extend(detections)\n",
    "\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "print('\\nSaving images:')\n",
    "for img_i, (imgs, detections) in enumerate(zip(imgs, img_detections)):\n",
    "\n",
    "    img = np.array(imgs)\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "\n",
    "    if detections is not None:\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "\n",
    "            print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "            if classes[int(cls_pred)] == 'person':\n",
    "                print('event')\n",
    "            \n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
    "                                    edgecolor=color,\n",
    "                                    facecolor='none')\n",
    "            ax.add_patch(bbox)\n",
    "            plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
    "                    bbox={'color': color, 'pad': 0})\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    plt.savefig('output/%d.png' % (img_i), bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
